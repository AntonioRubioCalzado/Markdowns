---
title: "Bayesian Model Packages"
author: "João Neto"
date: October 2014
output: 
  html_document:
    toc: true
    toc_depth: 3
    fig_width: 6
    fig_height: 6
cache: yes
---

## Bayesian First Aid

> Bayesian First Aid is an attempt at implementing reasonable Bayesian alternatives to the classical hypothesis tests in R. For the rationale behind Bayesian First Aid see the [original announcement](http://sumsar.net/blog/2014/01/bayesian-first-aid/). The development of Bayesian First Aid can be followed on [GitHub](https://github.com/rasmusab/bayesian_first_aid). Bayesian First Aid is a work in progress and I'm grateful for any suggestion on how to improve it! [ref](http://sumsar.net/blog/2014/01/bayesian-first-aid-binomial-test/)

The following code and tests in this chapter are taken from the package author's webpages, namely:

+ [Binomial Test](http://sumsar.net/blog/2014/01/bayesian-first-aid-binomial-test/): bayes.binom.test(x, n)
+ [One Sample and Paired Samples t-test](http://sumsar.net/blog/2014/02/bayesian-first-aid-one-sample-t-test/): bayes.t.test(x) 
+ [Pearson Correlation Test](http://sumsar.net/blog/2014/03/bayesian-first-aid-pearson-correlation-test/): bayes.cor.test(x, y)
+ [Test of Proportions](http://sumsar.net/blog/2014/06/bayesian-first-aid-prop-test/): bayes.prop.test(x, n) (check [part2](part2.html))
+ [Poisson test](http://sumsar.net/blog/2014/09/bayesian-first-aid-poisson-test/): bayes.poisson.test(x, T)

```{r, warning=FALSE, message=FALSE}
# To install:
## install.packages("devtools")
# library(devtools)
# install_github("rasmusab/bayesian_first_aid")

library(BayesianFirstAid)
```

### Binomial Test

We have $n$ trials with $x$ sucesses and $n-x$ failures. We assume that the tests are iid.

The likelihood, the distribution of the data is considered a Binomial with parameter $\theta$ ($n$ is considered fixed):

$$p(x|n, \theta) = {n \choose x} \theta^x (1-\theta)^{(n-x)}$$

The prior distribution of parameter $\theta$ used in the flat prior which is described by a Beta(1,1):

$$\theta \sim \text{Beta}(1,1)$$

The Bayesian estimation:

```{r}
# x: number of successes
# n: number of trials
# comp.theta: a fixed relative frequency of success to compare with the estimated relative frequency of success
# cred.mass: the amount of probability mass that will be contained in reported credible intervals
# n.iter: The number of iterations to run the MCMC sampling (default: 15000, max: 1e6-1)

model <- bayes.binom.test(x=9, n=11, comp.theta=0.7, cred.mass = 0.95)
model
```

Versus the classical binomial test:

```{r}
binom.test(x=9, n=11, p=0.7, conf.level=0.95)
```

To plot and summarize:

```{r}
summary(model)
plot(model)
```

The posterior predictive distribution is the distribution of unobserved observation $y$ (a prediction) conditional on the observed data $x$

$$p(y|x,n) = \int_{\theta} p(y|\theta) p(\theta|x,n) d\theta = E_{\theta|x,n} [ p(y|\theta) ]$$


`diagnostics` plots MCMC diagnostics based on the `code` package:

```{r}
diagnostics(model)
```

`model_code` print out R and JAGS code that runs the model (copy-paste friendly):

```{r}
model.code(model)
```

Eg, to change the prior just replace, in the model_string, the distribution of $\theta$ for something else (liek Jeffrey's prior `dbeta(0.5,0.5)`) and run the code yourself.

### One Sample and Paired Samples t-test

To replace the classical t-test, this test uses the [BEST](http://www.indiana.edu/~kruschke/BEST/) methodology which assumes the data $x=(x_1,x_2,\ldots,x_n)$ are distributed as a t distribution according to the following model:

$$x_i \sim t(\mu,\sigma,\nu$$

$$\mu \sim \mathcal{N}(M_{\mu}, S_{\mu})$$

$$\sigma \sim U(L_{\sigma}, H_{\sigma})$$

$$\nu \sim \text{ShiftedExp}(\frac{1}{29}, \text{shift}=1)$$

$\nu$ is the degrees of freedom of the t distribution (lower values mean heavier tails). The value $\frac{1}{29}$ is used to try to balance nearly normal distributions ($\nu\gt 30$) with heavy tailed distributions ($\nu\lt 30$).

For paired samples the test takes the difference between each paired sample and model just the paired differences using the one sample procedure.

As an eg let's load a dataset with coffee yields and compare these yields in two different time periords (1960-80 amd 1980-2001) and see if the test recognize some statistical significant change:

```{r}
d <- read.csv("roubik_2002_coffe_yield.csv")
head(d)
new.yield_61 <- d[,3][d$world=="new"]
new.yield_61
new.yield_81 <- d[,4][d$world=="new"]
new.yield_81

model <- bayes.t.test(new.yield_61, new.yield_81, paired=TRUE)
model
```

The $95$\% credible interval does not include zero, so the test recognizes a statistical difference between the two samples.

```{r}
summary(model)
plot(model)
```

The posterior predictive box presents a histogram with a smattering of t-distributions drawn from the posterior. If there is a large discrepancy between the model fit and the data then we need to think twice before proceeding. Herein it's ok.

```{r}
diagnostics(model)
model.code(model)
```

### Pearson Correlation Test

This is a measure of the linear correlation (dependence) between two variables $X_1$ and $X_2$, giving a value $\rho$ between +1 and -1 inclusive, where 1 is total positive correlation, 0 is no correlation, and -1 is total negative correlation

$$\rho = \frac{\text{cov}(X_1,X_2)}{\sigma_{X_1} \sigma_{X_2}$$

Classical statistical inference based on Pearson's correlation coefficient aims at test the null hypothesis that $\rho=0$ and finding the p\% confidence interval around $r$ (the sample correlation) that contains $\rho$.

The Pearson's correlation coefficient is  which assumes bivariate normality.

The model that a classical Pearson's correlation test assumes is that between two paired variables $X_{i1}$ and $X_{i2}$ it follows a bivariate normal distribution, ie, $X_i1 \sim \mathcal{N}(\mu_1,\sigma_1), X_i2 \sim \mathcal{N}(\mu_2,\sigma_2)$ and for each pair $(x_i,y_i)$ there is a linear dependency which is quantified by $\rho$. Here's an eg for $\rho=0.3$:

<img src="http://en.wikipedia.org/wiki/File:MultivariateNormal.png">

The classical model pressuposes that $X=(X_1,X_2)^T$ has a multivariate normal distribution:

$$X \sim \mathcal{N}(\mu,\Sigma)$$

where $\mu=(\mu_1,\mu_2)^T$ and

$$\Sigma = 
\left() \begin{array}{cc}
\sigma_1^2 & \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 & \sigma_2^2 \end{array} \right)$$

This test is implemented by R function `cor.test`.

The following Bayesian model replaces the normal for a bivariate t distribution which is more robust to outliers (it could also be a normal, of course, the Bayesian framework is much more flexible in this sense).

The Bayesian model becomes

$$X \sim t(\mu,\Sigma,nu)$$

$$\rho \sim U(-1,1)$$

$$\mu_x,\mu_y \sim \mathcal{N}(M_{\mu}, \S_{\mu})$$

$$\sigma_{x_1}, \sigma_{x_2}\sim U(L_{\sigma}, U_{\sigma})$$

$$\nu \sim \text{ShiftedExp}(\frac{1}{29}, \text{shift}=1)$$

Let's load some data:

```{r}
d <- read.csv("2d4d_hone_2012.csv")
head(d)
plot(d$ratio_2d4d, d$grip_kg, col=unclass(d$sex), pch=18)
legend("topright",c("female","male"), col=1:2, pch=18)
```

There is a visual difference between the two variables concerning gender. Let's try to find some pattern with this test

```{r}
model.male <- bayes.cor.test( ~ ratio_2d4d + grip_kg, data=d[d$sex=="male",], n.iter = 5000)
model.male
summary(model.male)
model.female <- bayes.cor.test( ~ ratio_2d4d + grip_kg, data=d[d$sex=="female",], n.iter = 5000)
model.female

plot(model.male)
plot(model.female)
```

Both estimates indicate a slight negative correlation (the males more than the females). The two ellipses show the 50% (darker blue) and 95% (lighter blue) highest density regions. The red histograms show the marginal distributions of the data with a smatter of marginal densities drawn from the posterior. Looking at this plot we see that the model fits quite well, however, we could be concerned with the right skewness of the `ratio_2d4d` marginal which is not captured by the model (the t-distribution is symmetric).

To take a look at the posterior difference in correlation between the male and the female group, we first extract the MCMC samples from the Bayesian First Aid fit object using the `as.data.frame` function:

```{r}
female.mcmc <- as.data.frame(model.female)
  male.mcmc <- as.data.frame(model.male)

hist(male.mcmc$rho-female.mcmc$rho, breaks=50, xlim=c(-1,1), main=bquote(paste("Density for ", rho, " male minus ", rho, " female")), yaxt="n", ylab="")
```

And the entire code:

```{r}
model.code(model.female)
```

For instance, if you wanted to replace $\rho \sim U(-1,1)$ with a Beta, such that $\rho \sim \text{Beta}(-1,1)$ we would replace

```{r, eval=FALSE}
rho ~ dunif(-1,1)
```

with

```{r, eval=FALSE}
rho_half_width ~ dbeta(1,1)
rho ~ 2*rho_half_width - 1  # shift and strech from [0,1] to[-1,1]
```

and replace the inits_lit variable:

```{r, eval=FALSE}
# from 
rho=cor(x, y, method="spearman")
# to
rho_half_width=( cor(x,y,method="spearman") + 1 ) / 2
```

By changing the parameter values of `dbeta` we could define several priors, placing more probability mass at that value of `rho` we know is more probable.

Goto [part2](part2.html)