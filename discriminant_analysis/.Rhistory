library(poweRlaw)
data(moby)
head(moby)
head(moby, n=12)
fit <- displ$new(moby)
fit
estimate_xmin(fit)
curve(dpareto(x,1.95,7),xlim=c(1,5),ylim=c(0,4),lwd=2,
main='Pareto PDFs',ylab='f(x|alpha,xm)')
curve(dpareto(x,1.95,7),xlim=c(1,5),ylim=c(0,4),lwd=2, main='Pareto pdf',ylab='f(x|alpha,xm)')
curve(dpareto(x,1.95,7),xlim=c(1,5),ylim=c(0,4),lwd=2, main='Pareto pdf',ylab='f(x|alpha,xm)', col="red")
plot(fit)
lines(fit)
est <- estimate_xmin(fit)
fit$setXmin(est)
lines(fit)
lines(fit, col="red")
fit
fit$xmin
fit$pars
plot(fit, pch=19)             # plotting the word's frequencies
plot(fit, pch=18)             # plotting the word's frequencies
lines(fit, col="red") # plot the fit
plot(fit, pch=21)             # plotting the word's frequencies
plot(fit, pch=1)             # plotting the word's frequencies
plot(fit, pch=1)             # plotting the word's frequencies
plot(fit, pch=2)             # plotting the word's frequencies
plot(fit)             # plotting the word's frequencies
lines(fit, col="red") # plot the fit
lines(fit, col="red", lwd=2) # plot the fit
ppareto(1,1.95,7)
ppareto(6,1.95,7)
ppareto(7,1.95,7)
ppareto(3,1.95,7)
ppareto(2,1.95,7)
qpareto(0.5,1.95,7)
qpareto(0,1.95,7)
qpareto(0.001,1.95,7)
qpareto(.99,1.95,7)
qpareto(.2,1.161,1)
qpareto(.8,1.161,1)
N <- length(moby)
N
dzipf(1,N,fit$pars)
dzipf(2,N,fit$pars)
dzipf(3,N,fit$pars)
dzipf(3,N,1/fit$pars)
dzipf(1,N,1/fit$pars)
moby[1]/sum(moby)
dzipf(10,N,1/fit$pars)
moby[10]/sum(moby)
xs <- 1:N
model <- dzipf(xs,N,1/fit$pars)
sum <- sum(moby)
sum
sum.moby <- sum(moby)
sum.moby
xs <- 1:N
N
model
data.model <- dzipf(xs,N,1/fit$pars)
xs <- 1:100
data.model <- dzipf(xs,N,1/fit$pars)
data.real <- moby[xs]/sum.moby
plot(data.model, data.real)
xs <- 1:1000
data.model <- dzipf(xs,N,1/fit$pars)
data.real <- moby[xs]/sum.moby
plot(data.model, data.real)
plot(data.model, data.real, xlim=c(0,.1), xlim=c(0,.1))
plot(data.model, data.real, xlim=c(0,.1), ylim=c(0,.1))
xs <- 100:1000
data.model <- dzipf(xs,N,1/fit$pars)
data.real <- moby[xs]/sum.moby
plot(data.model, data.real, xlim=c(0,.1), ylim=c(0,.1))
xs <- 1:1000
data.model <- dzipf(xs,N,fit$pars)
data.real <- moby[xs]/sum.moby
plot(data.model, data.real, xlim=c(0,.1), ylim=c(0,.1))
plot(data.model, data.real)#, xlim=c(0,.1), ylim=c(0,.1))
fit
int=-10.6513
bal=0.0055
-int/bal
install.packages("plotrix")
log(2)
(log(2)+int)/bal
(ln(2)+int)/bal
?log
(log(2)+int)/bal
(log(2)+int)/-bal
a = exp(-6+0.05*40+3.5)
a/(1+a)
2.5/.05
a = exp(-6+0.05*50+3.5)
a/(1+a)
require(ISLR)
names(Smarket)
summary(Smarket)
pairs(Smarket,col=Smarket$Direction)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
require(MASS)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit)
Smarket.2005=subset(Smarket,Year==2005)
Smarket.2005
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred[1:5,]
lda.pred
class(lda.pred)
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
library(MASS)
lda.model <- lda(V1 ~ . , data=iris, scale=TRUE, subset=train)
lda.model <- lda(Species ~ . , data=iris, scale=TRUE, subset=train)
lda.model <- lda(Species ~ . , data=iris, scale=TRUE)
lda.model$scaling
iris
iris[,-5]
transformed <- as.matrix(iris[,-5]) %*% lda.model$scaling
transformed
plot(transformed, col=iris$Species)
km <- kmeans(transformed, 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
library(stats)
set.seed(101)
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
km <- kmeans(transformed, 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
km <- kmeans(transformed, 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
km
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
set.seed(900)
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
set.seed(101)
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
lda.model <- lda(Species ~ . , data=iris, scale=TRUE)
lda.model$scaling
transformed <- as.matrix(iris[,-5]) %*% lda.model$scaling
transformed
table(predicted=km$cluster, true=iris$Species)
equire(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
# attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
# attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
setwd("~/Documents/Dropbox/My Work/Projects/R/My_Markdowns/discriminant_analysis")
my.data <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"),
header=FALSE)
head(my.data)
scale(my.data)
my.data <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"),
header=FALSE)
class(my.data)
my.data <-cbind(my.data[,1], scale(my.data[,2:14]))
my.data
head(my.data, n=10)
head(signif(my.data,3), n=10)
paste0("X",2:14)
names(X) <- paste0("X",2:14)
head(signif(my.data,3), n=10)
head(signif(X,3), n=10)
Y <- my.data[,1]
X <- scale(my.data[,2:14])
names(X) <- paste0("X",2:14)
head(signif(X,3), n=10)
class(X)
X <- as.data.frame(scale(my.data[,2:14]))
names(X) <- paste0("X",2:14)
head(signif(X,3), n=10)
names(X) <- paste0("X",1:13)
head(signif(X,3), n=10)
head(cbind(signif(X,3),Y), n=10)
train <- sample(1:dim(RawData)[1],50)
train
train <- sample(1:dim(X)[1],50)
train
result1 <- knn(X[train,], X[-train,], Y[train], k=1)
install.packages("class")
library(class)
result1 <- knn(X[train,], X[-train,], Y[train], k=1)
prediction <- knn(X[train,], X[-train,], Y[train], k=1)
table1 <- table(old=Y[-train], new=prediction)
table1
cbind(X,Y)
model <- lda(Y ~ . , data=cbind(X,Y), scale=TRUE, subset=train)
model
names(model)
model$prior
model$scaling # the resulting projection matrix to project our dataset into a 2-dimensional vector space
projected.data <- as.matrix(X) %*% model$scaling
projected.data
head(projected.data)
plot(projected.data, col=Y)
prediction <- knn(projected.data[train,], projected.data[-train,], Y[train], k=1)
table2 <- table(old=projected.data[-train], new=prediction)
table2 <- table(old=Y[-train], new=prediction)
table2
table1
train <- sample(1:dim(X)[1],50)  # training set
prediction <- knn(X[train,], X[-train,], Y[train], k=1)
table1 <- table(old=Y[-train], new=prediction)
table1
model <- lda(Y ~ . , data=cbind(X,Y), scale=TRUE, subset=train)
model$prior   # the prior probabilities used (by default, the class proportions for the training set)
model$scaling # the resulting projection matrix to project our dataset into a 2-dimensional vector space
projected.data <- as.matrix(X) %*% model$scaling
head(projected.data)
plot(projected.data, col=Y)
# Now let's reapply knn:
prediction <- knn(projected.data[train,], projected.data[-train,], Y[train], k=1)
table2 <- table(old=Y[-train], new=prediction)
table2
table1
set.seed(101)
# Read Sample Data
my.data <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"),
header=FALSE)
Y <- my.data[,1]                            # classes
X <- as.data.frame(scale(my.data[,2:14]))   # scaling data
names(X) <- paste0("X",1:13)
head(cbind(signif(X,3),Y), n=10)
train <- sample(1:dim(X)[1],50)  # training set
# First test with a knn clustering
library(class)
prediction <- knn(X[train,], X[-train,], Y[train], k=1)
table1 <- table(old=Y[-train], new=prediction)
table1
# Second test with a previous LDA to find the projection that best separates the data
model <- lda(Y ~ . , data=cbind(X,Y), scale=TRUE, subset=train)
model$prior   # the prior probabilities used (by default, the class proportions for the training set)
model$scaling # the resulting projection matrix to project our dataset into a 2-dimensional vector space
projected.data <- as.matrix(X) %*% model$scaling
head(projected.data)
plot(projected.data, col=Y)
# Now let's reapply knn:
prediction <- knn(projected.data[train,], projected.data[-train,], Y[train], k=1)
table2 <- table(old=Y[-train], new=prediction)
table2
table1
model <- lda(Y ~ . , data=cbind(X,Y), subset=train)
model$prior   # the prior probabilities used (by default, the class proportions for the training set)
model$scaling # the resulting projection matrix to project our dataset into a 2-dimensional vector space
projected.data <- as.matrix(X) %*% model$scaling
head(projected.data)
plot(projected.data, col=Y)
# Now let's reapply knn:
prediction <- knn(projected.data[train,], projected.data[-train,], Y[train], k=1)
table2 <- table(old=Y[-train], new=prediction)
table2
colMeans(X)  # faster version of apply(scaled.dat, 2, mean)
apply(X, 2, sd)
repmat <- function(data, nrows, ncols, byrow=T) {
ncols <- length(data) * ncols
rep <- matrix(rep(data, nrows), nrow=nrows, ncol=ncols, byrow=byrow)
return(rep)
}
data <- c(2,3,3,4,4,5,5,6,5,7,2,1,3,2,4,2,4,3,6,4,7,6)
y <- c(1,1,1,1,1,2,2,2,2,2,2);
X <- matrix(data, ncol=2, byrow=T)
mean <- colMeans(X)
Xm <- X - repmat(mean, nrow(X), 1)
C <- cov(Xm)
eig <- eigen(C)
z <- Xm%*%eig$vectors[,1]
p <- z%*%eig$vectors[,1]
p <- p + repmat(mean, nrow(p), 1)
plot(p[y==1,], col="red", xlim=c(0,8), ylim=c(0,8))
points(p[y==2,], col="green")
title(main="PCA projection", xlab="X", ylab="Y")
repmat(mean, nrow(X), 1)
mean
Xm <- X - repmat(mean, nrow(X), 1)
C
eig
data <- c(2,3,3,4,4,5,5,6,5,7,2,1,3,2,4,2,4,3,6,4,7,6)
y <- c(1,1,1,1,1,2,2,2,2,2,2);
X <- matrix(data, ncol=2, byrow=T)
X
Xm
scale(X)
Xm=scale(X)
C <- cov(Xm)
eig <- eigen(C)
z <- Xm%*%eig$vectors[,1]
p <- z%*%eig$vectors[,1]
p <- p + repmat(mean, nrow(p), 1)
plot(p[y==1,], col="red", xlim=c(0,8), ylim=c(0,8))
points(p[y==2,], col="green")
w
C <- cov(Xm)
eig <- eigen(C)
z <- Xm%*%eig$vectors[,1]
p <- z%*%eig$vectors[,1]
p <- p + repmat(mean, nrow(p), 1)
plot(p[y==1,], col="red", xlim=c(0,8), ylim=c(0,8))
points(p[y==2,], col="green")
?scale
scale(X,scale=FALSE)
Xm
Xm <- X - repmat(colMeans(X), nrow(X), 1)
Xm
scale(X,scale=FALSE)
eig
Xm <- scale(X, scale=FALSE)
C <- cov(Xm)
eig <- eigen( cov(Xm) )
z <- Xm%*%eig$vectors[,1]
p <- z%*%eig$vectors[,1]
p <- p + repmat(mean, nrow(p), 1)
plot(p[y==1,], col="red", xlim=c(0,8), ylim=c(0,8))
points(p[y==2,], col="green")
repmat(mean, nrow(p), 1)
p
z <- Xm %*% eig$vectors[,2]
p <- z %*% eig$vectors[,2]
p <- p + repmat(mean, nrow(p), 1)
plot(p[y==1,], col="red", xlim=c(0,8), ylim=c(0,8))
points(p[y==2,], col="green")
title(main="PCA projection", xlab="X", ylab="Y")
X <- matrix(c(2,3,3,4,4,5,5,6,5,7,2,1,3,2,4,2,4,3,6,4,7,6), ncol=2, byrow=T)
Y <- c(1,1,1,1,1,2,2,2,2,2,2);
plot(X,col=Y)
plot(X,col=Y, pch=19)
idx_class1 = Y[Y==1]
idx_class1
which(Y==1)
idx_class1 <- which(Y==1)
idx_class2 <- which(Y==2)
idx_class2
lda(Y ~ . , data=cbind(X,Y))
cbind(X,Y)
names(X) <- c("X1","X2")
X
my.data <- dataframe(X1=X[,1], X2=X[,2], Y=Y)
my.data <- data.frame(X1=X[,1], X2=X[,2], Y=Y)
lda(Y ~ . , data=my.data)
model <- lda(Y ~ . , data=my.data)
as.matrix(X) %*% model$scaling
plot(projected.data, col=Y)
X <- matrix(c(2,3,3,4,4,5,5,6,5,7,2,1,3,2,4,2,4,3,6,4,7,6), ncol=2, byrow=T)
Y <- c(1,1,1,1,1,2,2,2,2,2,2);
plot(X,col=Y, pch=19)
my.data <- data.frame(X1=X[,1], X2=X[,2], Y=Y)
model <- lda(Y ~ . , data=my.data)
projected.data <- as.matrix(X) %*% model$scaling
plot(projected.data, col=Y)
model
X
X <- matrix(c(2,3,3,4,4,5,5,6,5,7,2,1,3,2,4,2,4,3,6,4,7,6,1,6,2,7,1,7), ncol=2, byrow=T)
Y <- c(1,1,1,1,1,2,2,2,2,2,2,3,3,3);
plot(X,col=Y, pch=19)
my.data <- data.frame(X1=X[,1], X2=X[,2], Y=Y)
model <- lda(Y ~ . , data=my.data)
model
projected.data <- X %*% model$scaling
plot(projected.data, col=Y, pch=19)
model$scaling
new.data <- data.frame(X1=c(6.5), X2=c(6.5)) # should be class 3
new.data
predict(model, new.data)
new.data <- data.frame(X1=c(1.5),
X2=c(6.5)) # should be class 3
predict(model, new.data)
new.data <- data.frame(X1=c(2),
X2=c(6.5)) # should be class 3
predict(model, new.data)
new.data <- data.frame(X1=c(2),
X2=c(5)) # should be class 3
predict(model, new.data)
predict(model, new.data)$class
predict(model, new.data)$posterior
predict(model, new.data)$prior
model$prior
predict(model, new.data)$posterior
predict(model, new.data)$posterior[[1]]
predict(model, new.data)$posterior[1]
predict(model, new.data)$posterior
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class)
predict(model, new.data)$class
predict(model, new.data)$class[1]
new.data <- data.frame(X1=c(2),
X2=c(6)) # should be class 3
predict(model, new.data)$class
model$prior
predict(model, new.data)$posterior
plot(projected.data, col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class[1])
plot(projected.data, col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class[1])
plot(projected.data, col=Y, pch=19)
plot(X,col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class[1])
new.data <- data.frame(X1=c(2),
X2=c(5.5)) # should be class 3
predict(model, new.data)$class
model$prior
predict(model, new.data)$posterior
plot(X,col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class[1])
predict(model, new.data)$class[1]
predict(model, new.data)$class
plot(X,col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=predict(model, new.data)$class)
model
model$scaling
model$scaling[1,1]
model$scaling[1,2]
abline(model$scaling[1,1], model$scaling[1,2])
abline(a=model$scaling[1,1], b=model$scaling[1,2])
abline(1,1)
plot(projected.data, col=Y, pch=19)
abline(a=model$scaling[1,1], b=model$scaling[1,2])
abline(a=model$scaling[2,1], b=model$scaling[2,2])
plot(X,col=Y, pch=19)
abline(a=model$scaling[2,1], b=model$scaling[2,2])
plot(projected.data, col=Y, pch=19)
model
names(model)
model$lev
model$svd
model$N
model$means
plpot(model)
plot(model)
model
plot(X,col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=prediction$class)
model <- lda(Y ~ . , data=my.data)
model
projected.data <- X %*% model$scaling
plot(projected.data, col=Y, pch=19)
new.data <- data.frame(X1=c(2), X2=c(5.5)) # new data point should be class 3
prediction <- predict(model, new.data)
model$prior          # the prior probabilities before the new data point
prediction$posterior # the posterior probs for the classes given this new data point
plot(X,col=Y, pch=19)
points(new.data$X1, new.data$X2, pch=17, col=prediction$class)
