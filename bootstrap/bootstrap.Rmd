---
title: "Bayesian Networks"
author: "João Neto"
date: "May 2013"
output: 
  html_document:
    toc: true
    toc_depth: 3
    fig_width: 12
    fig_height: 6
cache: yes
---

> The basic idea of bootstrapping is that inference about a population from sample data (sample -> population) can be modeled by resampling the sample data and performing inference on (resample -> sample). As the population is unknown, the true error in a sample statistic against its population value is unknowable. In bootstrap resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference from resample data -> 'true' sample is measurable -- [wikipedia](http://en.wikipedia.org/wiki/Bootstrapping_(statistics))

This technique should be used when:
+ the theoretical distribution of a statistic of interest is complicated or unknown. Since the bootstrapping procedure is distribution-independent it provides an indirect method to assess the properties of the distribution underlying the sample and the parameters of interest that are derived from this distribution.
+ the sample size is insufficient for straightforward statistical inference. If the underlying distribution is well-known, bootstrapping provides a way to account for the distortions caused by the specific sample that may not be fully representative of the population.
+ power calculations have to be performed, and a small pilot sample is available. Most power and sample size calculations are heavily dependent on the standard deviation of the statistic of interest. If the estimate used is incorrect, the required sample size will also be wrong. One method to get an impression of the variation of the statistic is to use a small pilot sample and perform bootstrapping on it to get impression of the variance.

Standard Bootstrap
------------------

```{r}
set.seed(333)
statistic <- mean      # herein, the statistic we wish to compute will be mean
my.sample <- rnorm(30) # and we only have a small sample from some population 

N <- 15000

# execute bootstrap (resamplig from just the original sample):
boot.samples <- replicate(N, statistic(sample(my.sample, replace=TRUE)))
# compare it with N samples taken from the population:
real.samples <- replicate(N, statistic(sample(rnorm(30), replace=TRUE)))

plot(density(real.samples), ylim=c(0,2.5))
lines(density(boot.samples), col="red")
```

Using the same eg with `boot` package (more [info](http://www.statmethods.net/advstats/bootstrapping.html)):

```{r, warning=FALSE}
library(boot)

# boot() needs a function applying the statistic to x (the original data) over i, a vector of indexes
my.function <- function(x,i) { statistic(x[i]) }

boot.stat <- boot(my.sample, my.function, N)
boot.stat
# again we can plot it against the previous sample from the population
plot(density(real.samples), ylim=c(0,2.5))
lines(density(boot.stat$t), col="red")
boot.ci(boot.stat) # it's also possible to compute confidence intervals
```

Bayesian Bootstrap
------------------

> In standard bootstrapping observations are sampled with replacement. This implies that observation weights follow multinomial distribution. In Bayesian bootstrap multinomial distribution is replaced by Dirichlet distribution-- [http://rsnippets.blogspot.pt/2012/11/simple-bayesian-bootstrap.html](http://rsnippets.blogspot.pt/2012/11/simple-bayesian-bootstrap.html)

```{r, warning=FALSE,message=FALSE}
library(gtools)

set.seed(333)
N <- 1000

mean.bb <- function(x, n) {
  apply( rdirichlet(n, rep(1, length(x))), 1, weighted.mean, x = x )
}

boot.bayes <- mean.bb(my.sample, N)
plot(density(real.samples), ylim=c(0,2.5))
lines(density(boot.bayes), col="red")
quantile(boot.bayes, c(0.025, 0.975)) # find credible intervals
```

> [(Rubin (1981)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176345338) introduced the Bayesian bootstrap. In contrast to the frequentist bootstrap which simulates the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution.

> The data, X, are assumed to be independent and identically distributed (IID), and to be a representative sample of the larger (bootstrapped) population. Given that the data has N rows in one bootstrap replication, the row weights are sampled from a Dirichlet distribution with all N concentration parameters equal to 1 (a uniform distribution over an open standard N-1 simplex). The distributions of a parameter inferred from considering many samples of weights are interpretable as posterior distributions on that parameter -- LaplacesDemon helpfile

### Using `bayesboot`

This package from Rasmus Baath implements a Bayesian bootstrapping described [here](http://www.sumsar.net/blog/2015/07/easy-bayesian-bootstrap-in-r/):

```{r}
library(bayesboot)

boot.bayes2 <- bayesboot(my.sample, statistic)

plot(density(real.samples), ylim=c(0,2.5))
lines(density(boot.bayes2$V1), col="red")
summary(boot.bayes2)
```

To compare a statistic between two groups, we bootstrap each and compute the difference to calculate the posterior difference ([eg from here](http://www.sumsar.net/blog/2016/02/bayesboot-an-r-package/)):

```{r}
# Heights of the last ten American presidents in cm (Kennedy to Obama).
heights <- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185)

# The heights of opponents of American presidents (first time they were elected).
# From Richard Nixon to John McCain
heights_opponents <- c(182, 180, 180, 183, 177, 173, 188, 185, 175)

# Running the Bayesian bootstrap for both datasets
b_presidents <- bayesboot(heights,           statistic)
b_opponents  <- bayesboot(heights_opponents, statistic)

# Calculating the posterior difference and converting back to a 
# bayesboot object for pretty plotting.
b_diff <- as.bayesboot(b_presidents - b_opponents)
plot(b_diff)
```

It seems the presidential winner tends to have more height than his opponent.